# Image Captioning Using Transformer

This repository contains the code for an image captioning model using transformer-based architecture. The goal of this project is to generate captions for images by leveraging the capabilities of transformers, aiming to improve upon traditional image captioning methods that typically use convolutional and recurrent neural networks.

## Table of Contents
- [Overview](#overview)
- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Training and Evaluation](#training-and-evaluation)
- [Model](#model)
- [Requirements](#requirements)
- [Acknowledgments](#acknowledgments)

## Overview

Image captioning is a challenging problem at the intersection of computer vision and natural language processing. This project uses a transformer-based model to automatically generate descriptive captions for images, improving the accuracy of captions over previous CNN-RNN-based methods. 

## Features
- **Transformer-Based Model**: Utilizes transformer architecture instead of traditional CNN-RNN models.
- **Pretrained Models**: Option to load pretrained models or train from scratch.
- **Evaluation Metrics**: Includes metrics to assess the accuracy of generated captions.

## Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/abhi0706-source/Image_Captioning_Using_Transformer.git
   cd Image_Captioning_Using_Transformer
